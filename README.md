# RAG_with_LLMs_basic
## ğŸ§  Retrieval-Augmented Generation (RAG) with LangChain & TinyLlama

This project demonstrates how to build an **offline RAG (Retrieval-Augmented Generation)** pipeline using:

- ğŸ’¬ **Meta-llama/Llama-3.2-3B-Instruct** for local language modeling
- ğŸ§± **LangChain** for chaining retriever + prompt + LLM
- ğŸ“„ **PDF file ingestion and chunking**
- ğŸ” **Vector search with FAISS or Chroma**
- âš™ï¸ Optimized to run on CPU or GPU with 4-bit quantization (`bitsandbytes`)

---

## ğŸ“‚ Project Structure

<pre> project_root/ 
  â”œâ”€â”€ test.py # Entry point to run RAG loop 
  â”œâ”€â”€ src/ 
  |    â”œâ”€â”€ base/ 
  â”‚    â”‚    â””â”€â”€ llm_model.py # Load Llama + quantization 
  â”‚    â””â”€â”€ rag/ 
  â”‚         â”œâ”€â”€ main.py # Build RAG pipeline 
  â”‚         â”œâ”€â”€ file_loader.py # Load & chunk PDF files 
  â”‚         â”œâ”€â”€ vectorstore.py # FAISS/Chroma vector DB 
  â”‚         â”œâ”€â”€ offline_rag.py # Chain prompt â†’ LLM â†’ output parser 
  â”‚         â””â”€â”€ utils.py # Text extract utilities 
  â””â”€â”€ data_source/ 
       â””â”€â”€ generative_ai/ # Folder containing your PDF files </pre>

## â–¶ï¸ Run the system
python test.py
