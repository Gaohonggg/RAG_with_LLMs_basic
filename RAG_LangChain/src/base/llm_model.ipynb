{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae88da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fafe15",
   "metadata": {},
   "source": [
    "Khởi tạo quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8776ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20135b63",
   "metadata": {},
   "source": [
    "Khởi tạo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433458d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=\"\"\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e25b7",
   "metadata": {},
   "source": [
    "Khởi tạo tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f075ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "max_new_tokens = 1024\n",
    "\n",
    "print( tokenizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07772b",
   "metadata": {},
   "source": [
    "Khởi tạo model PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa6983e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x0000025C205054E0>\n"
     ]
    }
   ],
   "source": [
    "model_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,  # model đã khởi tạo\n",
    "    tokenizer=tokenizer,  # tokenizer đã khởi tạo\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print( model_pipeline )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497dd3e",
   "metadata": {},
   "source": [
    "Khởi tạo LLM PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a617774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFacePipeline\u001b[0m\n",
      "Params: {'model_id': 'gpt2', 'model_kwargs': {'temperature': 0.9}, 'pipeline_kwargs': None}\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "    \"temperature\": 0.9,  # Càng tăng số này, độ sáng tạo càng cao\n",
    "}\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=model_pipeline,\n",
    "    model_kwargs=gen_kwargs,\n",
    ")\n",
    "\n",
    "print( llm )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_rs_ubuntu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
